\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Project Title: System Verification and Validation Plan for \progname{}} 
\author{Author Name}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations or acronyms -- you can simply reference the SRS
  \citep{SRS} tables, if appropriate}

\newpage

\pagenumbering{arabic}

This document ... \wss{provide an introductory blurb and roadmap of the
  Verification and Validation plan}

\section{General Information}

\subsection{Summary}

\wss{Say what software is being tested.  Give its name and a brief overview of
  its general functions.}

\subsection{Objectives}

\wss{State what is intended to be accomplished.  The objective will be around
  the qualities that are most important for your project.  You might have
  something like: ``build confidence in the software correctness,''
  ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
  just those that are most important.}

\subsection{Relevant Documentation}

\wss{Reference relevant documentation.  This will definitely include your SRS
  and your other project documents (MG, MIS, etc).  You can include these even
  before they are written, since by the time the project is done, they will be
  written.}

\citet{SRS}

\section{Plan}

\wss{Introduce this section.   You can provide a roadmap of the sections to
  come.}

\subsection{Verification and Validation Team}

\wss{You, your classmates and the course instructor.  Maybe your supervisor.
  You shoud do more than list names.  You should say what each person's role is
  for the project.  A table is a good way to summarize this information.}

\subsection{SRS Verification Plan}

\wss{List any approaches you intend to use for SRS verification.  This may just
  be ad hoc feedback from reviewers, like your classmates, or you may have
  something more rigorous/systematic in mind..}

\wss{Remember you have an SRS checklist}

\subsection{Design Verification Plan}

\wss{Plans for design verification}

\wss{The review will include reviews by your classmates}

\wss{Remember you have MG and MIS checklists}

\subsection{Implementation Verification Plan}

\wss{You should at least point to the tests listed in this document and the unit
  testing plan.}

\wss{In this section you would also give any details of any plans for static verification of
  the implementation.  Potential techniques include code walkthroughs, code
  inspection, static analyzers, etc.}

\subsection{Automated Testing and Verification Tools}

\wss{What tools are you using for automated testing.  Likely a unit testing
  framework and maybe a profiling tool, like ValGrind.  Other possible tools
  include a static analyzer, make, continuous integration tools, test coverage
  tools, etc.  Explain your plans for summarizing code coverage metrics.
  Linters are another important class of tools.  For the programming language
  you select, you should look at the available linters.  There may also be tools
  that verify that coding standards have been respected, like flake9 for
  Python.}

\wss{The details of this section will likely evolve as you get closer to the
  implementation.}

\subsection{Software Validation Plan}

\wss{If there is any external data that can be used for validation, you should
  point to it here.  If there are no plans for validation, you should state that
  here.}

\section{System Test Description}
	
\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good.}

\subsubsection{Area of Testing1}

\wss{It would be nice to have a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good.  If a section
  covers tests for input constraints, you should reference the data constraints
  table in the SRS.}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}
					
How test will be performed: 
					
\item{test-id2\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the
  appropriate functional tests from above.  The test cases should mention
  reporting the relative error for these tests.}

\wss{Tests related to usability could include conducting a usability test and
  survey.}

\subsubsection{Area of Testing1}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Type: 
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}


\section{Unit Test Description}

The pytest library will be used to complete unit testing for this toolbox. To develop unit tests for the internal functions of the program, we will be creating a corresponding test file for each module. Each test file will contain unit tests for each function within the module. These tests contain a variety of inputs, including those which output the correct transformation as well as inputs that generate errors and exceptions. \\

\noindent We will be using coverage metrics to determine how well-tested our code is. This will be determined through the use of coverage.py, a python library that quickly analyzes code coverage of all modules within a project. We will be aiming for 90\% code coverage per module, ensuring that we adequately test all functions. 
%\wss{Reference your MIS (detailed design document) and explain your overall
 % philosophy for test case selection.}  
%\wss{This section should not be filled in until after the MIS (detailed design
  %document) has been completed.}

\subsection{Unit Testing Scope}

Route choice analysis variable modules will be verified for correct functionality (correct sample inputs output correct sample outputs), but logic of previously existing modules will be assessed for correctness by our supervisor, Dr. Paez.
%\wss{What modules are outside of the scope.  If there are modules that are
%  developed by someone else, then you would say here if you aren't planning on
%  verifying them.  There may also be modules that are part of your software, but
%  have a lower priority for verification than others.  If this is the case,
% 5 explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

This section will be completed once the MIS has been updated and there is greater clarity on specific modules. 

\subsection{Traceability Between Test Cases and Modules}
This section will be completed once the MIS has been updated and there is greater clarity on specific modules. 
%\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section*{Appendix --- Reflection}
General Team:\\

\noindent To implement the verification and validation plan within our project our team will have to learn a few new skills. The team will have to create a standard testing suite and develop a standard testing method that each member of the team will follow. This will be to ensure that all tests are understandable and readable by all members of the group. \\

\noindent The team will also familiarize themselves with the pytest framework which will allow us to create consistent, efficient tests that will test each function in our program. As well as learn system testing techniques such as ---.  The team will also need to create a testing strategy that is appropriate and feasible for the project.\\

\noindent Smita Singh:\\ Smita will be responsible for creating unit test for Route Choice Analysis. She will need understand the inputs and outputs of each of the methods that will be required to perform that specific module. Smita will also be leading the creation of a test strategy. \\

\noindent Moksha Srinivasan:\\ Similar to Smita, Moksha will also be responsible for creating tests for Route Choice Analysis. Moksha will also be responsible for helping set up the standard test suite and implementing CI/CD into our git repository by following the tutorial given by Chris Shankula. \\

\noindent Longwei Ye:\\
Longwei will be responsible for creating unit testing for trip trajectory. Longwei will be responsible for learning about best testing practises through research and will ensure that the team sticks to those practises.\\

\noindent Niyatha Rangarajan:\\
Niyatha will be creating unit testing module for travel episode verification and categorization. Niyatha has been passionate about end to end system testing. She will be researching how to perform relevant system testing by researching industry standards and then be responsible for informing the rest of the team.\\

\noindent Abeer Alyasiri:\\ Abeer will be responsible for learning about different file formats (CSV, XML, JSON, SHP) and the most efficient ways of parsing through and transforming data. This will ensure that modules are well designed and time efficient. She will ensure test cases include all relevant input file formats and malformed data inputs as well. She will learn about these best practices through the use of data parsing python tutorials online as well as researching libraries/prior implementations of open source GIS analysis tools. Through her co-op position, Abeer has significant experience working with various types of data and hence is the most suited member of our team for this task.   \\

\noindent Nicholas Lobo: \\
Nicholas has always been interested in learning about data analysis and normalization. Within the scope of this project, he has taken on the responsibility of learning about GPS data standards to help provide a wide range of inputs for all tests. He will ensure that the preprocessing unit can handle various types of GPS data as well as inform decisions about edge cases related to incorrect data. He can complete this task by consulting academic papers that detail the characteristics of and how to parse GPS data.\\


\end{document}
