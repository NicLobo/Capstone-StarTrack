\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}
\usepackage{multirow}

%\input{../Comments}
%\input{../Common}

\begin{document}

\title{Project Title: System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

\newpage

\tableofcontents

\listoftables
% \wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l p{10cm}} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  SRS & Software Requirements Specification \\
%   GPS & Global Positioning Systems\\
%   GIS & Geographical Information Systems\\
%   GERT & GIS-based episode reconstruction toolkit \\
%   Point & location coordinate with time stamp.\\
%   Session & Object activity history quantified by GPS points \\
%   Episode & Session\\
%   Segment & Group of GPS Points combined based on episode attributes.\\
%   Trip & GPS points represents an object moving to a different position.\\
%   Route & Object path to get from position A to position B\\
%   Mode Detection (MD) & Detection of type of transportation being used \\
%   Time Use Diary (TUD) & Time Use Diary are records of continuous events and actions through a particular period of time (usually 24 to 48 hours) \\
%   Route Choice Analysis (RCA) &  Analyzes route selection from point a to point b\\
%   .shp & .shp are geospatial data format files\\
%   CSV/.csv & Comma Separated Values is a file type that contains large amounts of data separated by commas. \\
%   Potential Activity Locations (PALS) & PALS are potential trip stops \\
%   Activity Locations (ALs) & ALs are trip stops \\


  \bottomrule
\end{tabular}\\

% \wss{symbols, abbreviations or acronyms --- you can simply reference the SRS
%   \citep{SRS} tables, if appropriate}

% \wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

This document provides the project's verification and 
validation plan for documentation phase and implementation phase. Also, it will include detailed testing decisions for system test cases and unit test cases.  
% \wss{provide an introductory blurb and roadmap of the
%   Verification and Validation plan}

\section{General Information}

\subsection{Summary}
The software being verified and validated is the \emph{yoGERT} toolbox. The software general functions include:
\begin{itemize}
    \item Process user's GPS files into compatible data types.
    \item Determine choice model estimations.
    \item Extract travel episodes variables.
    \item Extract segments from travel episodes and classify segments.
    \item Categorize movement and stop behaviour. 
\end{itemize}

% \wss{Say what software is being tested.  Give its name and a brief overview of
%   its general functions.}

\subsection{Objectives}
The objectives of the verification and validation plan are:
\begin{itemize}
    \item To build confidence in the toolbox correctness. By confirming the functions are executed as expected by the requirements. 
    \item To demonstrate the fundamental functions meet the stakeholders goals.
    \item To demonstrate the project scope meets the capstone deadline. 
    \item To build confidence in toolbox accessibility and transferability. 
\end{itemize}

% \wss{State what is intended to be accomplished.  The objective will be around
%   the qualities that are most important for your project.  You might have
%   something like: ``build confidence in the software correctness,''
%   ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
%   just those that are most important.}

\subsection{Relevant Documentation}
The test plan are created follow the documents listed below:
\begin{itemize}
    \item Software Requirements Specification.
    \item Module Guide.
    \item Module Interface Specification.
\end{itemize}


% \wss{Reference relevant documentation.  This will definitely include your SRS
%   and your other project documents (design documents, like MG, MIS, etc).  You
%   can include these even before they are written, since by the time the project
%   is done, they will be written.}

% \citet{SRS}

\section{Plan}

% \wss{Introduce this section.   You can provide a roadmap of the sections to
%   come.}
  
This section outlines verification and validation plan including details on possible testing approaches and division of resources. The section provides rationalized decisions making process for the verification and validation plan.

\subsection{Verification and Validation Team}

% \wss{Your teammates.  Maybe your supervisor.
%   You shoud do more than list names.  You should say what each person's role is
%   for the project's verification.  A table is a good way to summarize this information.}
The testing team consists of all members of \emph{yoGERT} team. All team members
need to be actively aware of each testing plan and involved in all aspects of the
testing process. All team members are involved because this is a capstone project
that requires students to participate in all the project's stages. Therefore, the
team aims to evenly split testing and preparing for automated testing. The table
below shows the division of responsibilities. It assigns leaders for different
testing milestones. This way the team is sure that every testing stage is on
track.  
\newpage
\begin{table}[h!]
    \begin{tabular}{|c|p{50mm}|p{50mm}| }
 \hline
 \textbf{Team Member} & \textbf{Testing Role} & \textbf{Responsibilities}  \\ 
 \hline
 Abeer Alyasiri & SRS and Design Verification and Acceptance Testing & Leads document and code walkthroughs and inspections. Leads user and business acceptance testing using black box techniques. \\ 
 Longwei Ye & Integration Testing & Leads regression testing using black box techniques. \\ 
 Moksha Srinivasan & Unit Testing & Leads implementation of white box testing techniques. \\
 Smita Singh & Unit Testing & Leads implementation of white box testing techniques. \\
 Nicholas Lobo & System Testing & Leads functional requirements testing. \\
 Niyatha Rangarajan & System Testing & Leads non-functional requirements testing. \\
 \hline
\end{tabular}
    \caption{Verification and Validation Team.}
    \label{tab:my_label}
\end{table}

\subsection{SRS Verification Plan}

% \wss{List any approaches you intend to use for SRS verification.  This may include
%   ad hoc feedback from reviewers, like your classmates, or you may plan for 
%   something more rigorous/systematic.}
The SRS verification plan consists of three parts. The main objectives is to check
if the SRS document was completed according to the Volere Template Standards and 
if the requirements address the project goals. All parts will utilize a static 
testing technique that includes structured and unstructured reviews, walkthroughs or checklists.\\
Part one is the unstructured feedback received from 
classmates in the form of GitHub issues. These reviews provide technical
improvements on the document from outside the team. It is helpful because it 
improves the document's information flow to professional individuals in the
field, such as software engineers.\\ 
Part two is the structured review received from the TA. The TA follows a checklist in the form of a rubric. The feedback is beneficial because the SRS is reviewed
from an industry standard perspective. Hence, the document will be
closer to implementing best-practices techniques throughout the
document. This increases the productivity of using the SRS during the
design stage.\\ 
Part three is a structured review with the supervisor. The review will be a
combination of SRS walkthrough and modified task based inspection. 
The objectives of the walkthrough is to introduce the supervisor to the 
team's documentation and receive general feedback on the scope and 
clarity of the documentation. On the other hand the task based 
inspection will analyze both functional and non-functional
requirements in depth. The inspection will consist of questions to 
motivate the supervisor to think about the relationship between 
system goals and the formulated requirements. This is helpful with 
removing ambiguities of the requirement's relevance to the desired final system. 
Also, the inspection will focus on problem categorization.
These will represent the talking points of the task based inspection with the supervisor.
The categorizations are clarity of requirements, conflicting requirements, and unrealistic requirements problems. \\
All reviews collected from the SRS verification plan will be applied
to the document before the design document deliverable.

% \wss{Maybe create an SRS checklist?}

\subsection{Design Verification Plan}

Design verification will be similar to the SRS verification part one and
part two. In addition it will include a formal review by teammates using
a checklist. The checklist will consist of Dr. Spencer's MG and MIS checklists and the following points:
\begin{itemize}
    \item Each design decision maps to one or more requirements. 
    \item Each design specification has one output. 
    \item Each function decomposition follow top to down design model.
    \item Design specification connect functional processes logically for the user to carry out tasks.
    \item Design specification does not include implementation details.
    \item Design specification describe inputs, logical operations, and output. 
    \item Design specification outputs are consistent across a division of input cases.
    \item Design specification outlines error responses for unexpected behaviour. 
\end{itemize}

The team will conduct the verification against the checklist using 
static and dynamic testing techniques. Static testing will involve 
a walkthrough to proof traceability and accuracy of the system 
architectural model. Dynamic testing will include unit testing, white box testing, black box testing, and integration testing.

% \wss{Plans for design verification}

% \wss{The review will include reviews by your classmates}

% \wss{Create a checklists?}

\subsection{Verification and Validation Plan Verification Plan}

Verification and validation plan will be verified through reviews. It is important to highlight that it is difficult to proof the correctness of the test cases. Therefore, the combination multiple verification techniques induct that the verification and validation plan  approximately tested critical points of the system.\\ 
First, it will be verified against Dr. Spencer's VnV-checklist by \emph{yoGERT} team members. It will verify the completeness of the test cases. It is done by tracing at least one requirement to a test case and examining the requirement across different types of inputs.\\ 
Second, it will be reviewed by classmates in an informal way. This feedback is beneficial because it is outside the team professional opinion on what information is missing from the document.\\ 
Third, it will be reviewed by the TA in a standard way using the rubric as a checklist. The review will focus on the accuracy of information used to formulate the plan and if the plan is appropriate to the project. The plan is appropriate if it is feasible within the capstone timeline and test cases are complete within its requirement scope. 

% \wss{The verification and validation plan is an artifact that should also be verified.}

% \wss{The review will include reviews by your classmates}

% \wss{Create a checklists?}

\subsection{Implementation Verification Plan}

% \wss{You should at least point to the tests listed in this document and the unit
%   testing plan.}
The implementation verification plan includes both
system test cases and unit test cases listed in this 
document. The verification plan is a combination of 
different testing techniques to start with testing the 
building blocks of the system up to testing structural 
interaction between theses components.\\ 
In the early stages of the implementation verification 
plan, the team will conduct static verification
techniques. It includes code inspections to test code 
readability and code walkthroughs to verify 
implementation meets that design specification. \\
The other stages of the implementation verification plan will rely on dynamic 
testing techniques. These tests will be driven by white box testing and 
integration testing techniques. Both these techniques are focused on proving that 
the system follows the design specification and is consistent with the addition of
new components. Also, The verification plan must encapsulate testing scenarios of how the system reacts to faulty inputs. It can be tested by inputting irrational data points and observing if a safe output will be produced instead of system failure. Therefore, it is important that the testing cases will include boundary and edge inputs to the system's safe outputs and consistent behaviour.


% \wss{In this section you would also give any details of any plans for static verification of
%   the implementation.  Potential techniques include code walkthroughs, code
%   inspection, static analyzers, etc.}

\subsection{Automated Testing and Verification Tools}

% \wss{What tools are you using for automated testing.  Likely a unit testing
%   framework and maybe a profiling tool, like ValGrind.  Other possible tools
%   include a static analyzer, make, continuous integration tools, test coverage
%   tools, etc.  Explain your plans for summarizing code coverage metrics.
%   Linters are another important class of tools.  For the programming language
%   you select, you should look at the available linters.  There may also be tools
%   that verify that coding standards have been respected, like flake9 for
%   Python.}

% \wss{If you have already done this in the development plan, you can point to
% that document.}

% \wss{The details of this section will likely evolve as you get closer to the
%   implementation.}

The section was done in the development plan document Sections 6 and 7. \\

The details of this section will likely evolve further
in the project. Currently the plan is to only use 
automation testing for unit tests. 

\subsection{Software Validation Plan}

% \wss{If there is any external data that can be used for validation, you should
%   point to it here.  If there are no plans for validation, you should state that
%   here.}
  
Software validation plan will be divided into two parts. Part one will involve walkthrough and task based inspection with the supervisor similar to the SRS 
verification plan section. It will be conducted, for the same reasons from before, to flush out any problems with the SRS requirements. This standardized review will be conducted prior to implementation. Part two will involve walkthrough and demonstration to the supervisor to validate that the system behaves as the primary stakeholder expected. The formal walkthrough ensures validation of the design implementation functionality. On the other hand the demonstration validate the system's usability and response to user inputs. The supervisor will be able to provide feedback as he understand the GIS toolbox functionality and he represents a typical user for the \emph{yoGERT} toolbox. 
If time permits, external data can be used for validation. The external data will be ARC GIS outputs to the same inputs fed into the \emph{yoGERT} toolbox. The objective of this validation is to show the consistency between the \emph{yoGERT} toolbox and the current available toolbox. This form of validation need to use external data with exact method applied to since the \emph{yoGERT} toolbox is implementing parts of the ARC GIS application. Hence, not all outputs of the ARC GIS application are the expected outputs from the \emph{yoGERT} toolbox.

% \wss{You might want to use review sessions with the stakeholder to check that
% the requirements document captures the right requirements.  Maybe task based
% inspection?}

% \wss{This section might reference back to the SRS verification section.}


\section{System Test Description}
	
\subsection{Tests for Functional Requirements}

The testing of functional requirements will be divided into two sections. One to test user functionality and one to test system functionality. 

\subsubsection{User Functionality Tests}

This will test all functionality from the users perspectives. How data is being read will be the main focus of these tests. 
		
\paragraph{User Input Testing}

\begin{enumerate}

\item{test-UT-1\\}

Control: Manual 
					
Initial State: The application has been loaded onto the computer
					
Input: User loads in a CVS file of GPS data
					
Output: The system saves the CVS file of GPS data

Test Case Derivation: The user wants the application to read the given CSV file 
					
How test will be performed: Different sets of valid CSV data will be uploaded by the tester to see if the computer reads the values correctly

Associated Functional Requirement: R1 

\item{test-UT-2\\}

Control: Manual 
					
Initial State: The application has been loaded onto the computer
					
Input: The system has a loaded file of GPS data 
					
Output: The system saves the values found in the CSV file as latitude longitude and time variables

Test Case Derivation: The user wants to use the software to save the given CVS files into variables that can be manipulated
					
How test will be performed: Different sets of valid CSV's of GPS data will be uploaded by the tester to see if the computer reads the values correctly

Associated Functional Requirement: R2,R5

\item{test-UT-3\\}

Control: Manual 
					
Initial State: The application has been loaded onto the computer
					
Input: User loads in a CVS file of TUD
					
Output: The system saves the the the CSV file of TUD

Test Case Derivation: The user wants to use the software to read the given CSV file and save it to attributes that can be 
					
How test will be performed: Different sets of valid CSV's of TUD data will be uploaded by the tester to see if the computer reads the values correctly

Associated Functional Requirement: R3

\item{test-UT-4\\}

Control: Manual 
					
Initial State: A CSV of GPS data has been inputted to the application
					
Input: User downloads the file that it uploaded to the system 
					
Output: The system gives the user the data in a CSV format 

Test Case Derivation: The user wants to use the software to read the given CSV file and save it to attributes that can be 
					
How test will be performed: Different sets of valid CSV's of TUD data will be uploaded by the tester to see if the computer reads the values correctly

Associated Functional Requirement: R4
\end{enumerate}

\subsubsection{System Functionality Tests}

This will test all functionality from the system perspectives. How data is being process and outputted will be the main focus of these tests. 
		
\paragraph{System Output Testing}

\begin{enumerate}

\item{test-ST-1\\}

Control: Manual
					
Initial State: CSV of GPS data has been inputted to the application
					
Input: The user types a function to call for the system to organize the inputted data into episodes 
					
Output: The system returns a report of categorized data points such as speed, duration, distance, and change in direction.

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: The tester will use a variety of CSV files filled with valid GPS data and use the function call to see if valid reports were generated


Associated Functional Requirement: R6



\item{test-ST-2\\}

Control: Manual
					
Initial State: CSV of TUD data has been inputted to the application
					
Input: The user types a function to call for the system to organize the inputted data into episodes 
					
Output: The system returns a report which contains a list of episodes that have categorized data points such as speed, duration, distance, and change in direction.

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: The tester will use a variety of CSV files filled with valid GPS data and use the function call to see if valid reports were generated


Associated Functional Requirement: R7

\item{test-ST-3\\}

Control: Manual
					
Initial State: CSV of GPS data has been inputted to the application
					
Input: The user types a function to call for the system to organize the inputted data into episodes 
					
Output: The system returns a report of episodes categorized by different  methods of transportation(walk, car, bus).

Test Case Derivation: The user wants to understand the methods of travel used from the set of data points given

How test will be performed: The tester will use a variety of CSV files filled with valid GPS data and use the function call to see if valid categories are found in the reports generated


Associated Functional Requirement: R8


\item{test-ST-4\\}

Control: Manual
					
Initial State: CSV of GPS data has been inputted to the application and a report of episodes was generated 
					
Input: The user selects one of the episodes generated from the report
					
Output: The system returns the segments of the episodes into type stop and trip 

Test Case Derivation: The user wants to understand the behaviour of the object given an episode in the report

How test will be performed: The tester will use a variety of generated reports to see if valid episode segments were created 


Associated Functional Requirement: R9


\item{test-ST-5\\}

Control: Manual
					
Initial State: CSV of GPS data has been inputted to the application 
					
Input: The report of episodes and segments are generated
					
Output: The system generates the trip trajectory values based on the given segments

Test Case Derivation: The system needs trip trajectory values for route choice analysis

How test will be performed: The tester will validate the trajectory values based on the given CSV GPS data 

Associated Functional Requirement: R10

\item{test-ST-5\\}

Control: Manual
					
Initial State: CSV of GPS data has been inputted to the application and the report of episodes are generated
					
Input: The report of episodes and segments are generated
					
Output: The system generates activity locations for each of the episodes in the report

Test Case Derivation: The system needs to generate high and low activity locations

How test will be performed: The tester will validate the trajectory values based on the given CSV GPS data 

Associated Functional Requirement: R11

\item{test-ST-5\\}

Control: Manual
					
Initial State: CSV of GPS data has been inputted to the application 
					
Input: The report of episodes and segments are generated
					
Output: The system generates the trip trajectory values based on the given segments

Test Case Derivation: The system needs trip trajectory values for route choice analysis

How test will be performed: The tester will validate the trajectory values based on the given CSV gps data 

Associated Functional Requirement: R11
\item{test-ST-5\\}

Control: Manual
					
Initial State: CSV of GPS data has been inputted to the application 
					
Input: The report of episodes and segments are generated
					
Output: The system generates the trip trajectory values based on the given segments

Test Case Derivation: The system needs trip trajectory values for route choice analysis

How test will be performed: The tester will validate the trajectory values based on the given CSV gps data 

Associated Functional Requirement: R11

\item{test-ST-5\\}

Control: Manual
					
Initial State: CSV of GPS data has been inputted to the application 
					
Input: The report of episodes and segments are generated
					
Output: The system generates the trip trajectory values based on the given segments

Test Case Derivation: The system needs trip trajectory values for route choice analysis

How test will be performed: The tester will validate the trajectory values based on the given CSV gps data 

Associated Functional Requirement: R11


\end{enumerate}



\subsection{Tests for Nonfunctional Requirements}

\paragraph{* Storing user information is now considered as a stretch goal. This conclusion was reached after a dicussion with our stakeholders. Hence, the SRS will be revised to avoid including such information. The following tests do not involve storing user information while the rest of SRS NFR related information remains unchanged. }
\subsubsection{UI/UX components}
\paragraph{Since the information must be presented to the user in an understandable manner, we must have UI/UX related tests.}

\begin{enumerate}

\item{test-id1\\}

Type: Regression testing
					
Initial State: Word document is present in the directory. Various functions to perform on the GPS data are shown to the user.
					
Input/Condition: The user types a function to call for the system to organize the
inputted data (word document) into episodes giving a word document as input. 
					
Output/Result: An error stating that the input file provided is not of the correct format.
					
How test will be performed: Since, different functions require different inputs, it is important to see if the current modules functionality changes when the format of the input file changes.

Associated NFR: 11
\item{test-id2\\}

Type: Manual testing
					
Initial State: CSV of GPS data has been inputted to the application
					
Input: The user types a function to call for the system to organize the
inputted data into episodes
					
Output: The system returns a possible set of input data types if the function matches a stored function in the system
					
How test will be performed: This works like VS code, were as you type a function, a function description hovers over the function call, depciting the required user input for that function. One can try this test with different function calls to check its validity.

Associated NFR: 1, 2, 3, 4, 5, 6, 7 
\end{enumerate}

\subsubsection{Memory and performance issues}
\paragraph{Since, we are working with large sets of data, we must make sure the edge cases of data size and its relevant processing time are considered. }

\begin{enumerate}

\item{test-id3\\}

Type: Regression testing
					
Initial State: CSV of 47.3 million data points of GPS data has been inputted to the application
					
Input/Condition: The user types a function to call for the system to organize the
inputted data (word document) into episodes giving a word document as input. 
					
Output/Result: The system returns a report of categorized data points such
as speed, duration, distance, and change in direction within 6000 seconds upon request
					
How test will be performed: We perform edge case tests to see if performance and capacity requirements are met. 

Associated NFR: 9, 11, 12

\item{test-id4\\}

Type: Unit testing
					
Initial State: CSV of GPS data has been inputted to the application
					
Input: The user types a function to call for the system to organize the
inputted data into episodes
					
Output: The system returns a possible set of input data types if the function matches a stored function in the system
					
How test will be performed: We perform a unit test to see if the outputted data matches the expected episodes we require from the system. This is helpful for precision requirements.

Associated NFR: 11

\end{enumerate}


\subsubsection{Security of user information}
		
\paragraph{Since the information inputted will be used by APIs online, we must ensure protection of user information.}

\begin{enumerate}

\item{test-id5\\}

Type: Manual
					
Initial State: CSV of GPS data has been inputted to the application
					
Input/Condition: The user types a function to call for the system to organize the
inputted data into episodes
					
Output/Result: No data seen at API endpoint.
					
How test will be performed: We must make sure we use online APIs like pandas, geopy, etc. does not store any user inputted information.

Associated NFR: 19

\item{test-id6\\}

Type: Dynamic testing
					
Initial State: CSV of GPS data has been inputted to the application
					
Input: The user types a function to call for the system to organize the
inputted data into episodes
					
Output: Inputted data has not changed once the episodes are created.
					
How test will be performed: Black box testing using Finite state machines. If there is no change of state for the input, then the test succeeds. 

Associated NFR: 10

\end{enumerate}

\subsubsection{Environment issues}
		
\paragraph{For the functioning of the application, it must have certain prerequisite software like Python installed and the environment it is run on like Mac, Windows, etc. must be accounted for.}

\begin{enumerate}

\item{test-id7\\}

Type: Unit testing
					
Initial State: CSV of GPS data has been inputted to the application
					
Input/Condition: The user types a function to call for the system to organize the
inputted data into episodes. 
					
Output/Result: Error is outputted stating that Python must be installed in the system.
					
How test will be performed: Python is not installed in the system before inputting the data. 

Associated NFR: 15

\item{test-id8\\}

Type: Unit testing
					
Initial State: CSV of GPS data has been inputted to the application
					
Input/Condition: The user types a function to call for the system to organize the
inputted data into episodes. 
					
Output/Result: The system returns a report of categorized data points such
as speed, duration, distance, and change in direction
					
How test will be performed: Python2 is installed in the system before inputting the data. Since, Python can only be installed on a valid OS, we simultaneously test for the operational environment.

Associated NFR: 14,15,16

\end{enumerate}
\subsection{Traceability Between Test Cases and Requirements}

\paragraph{*8,13,17,18 NFRs relate to stretch goals like storing the user information and building a GUI. Hence, based on reassessment of nfrs with stakeholders, these nfrs will be reevaluated or omitted in the revised version of the SRS.}
\begin{table}[H]
\centering
\scalebox{0.5}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline        
& NFR1 & NFR2 & NFR3 & NFR4 & NFR5 & NFR6 & NFR7& NFR8 & NFR9 & NFR10 & NFR11 & NFR12 & NFR13 & NFR14 & NFR15 & NFR16 & NFR17 & NFR18 & NFR19\\ \hline
test-id1  & & & & & & & & & & &X & & & &  & & & & \\ \hline
test-id2  & &X &X &X &X &X &X & & & & & & & &  & & & & \\ \hline
test-id3  & & & & & & & & &X & &X &X & & &  & & & & \\ \hline
test-id4  & & & & & & & & & & &X & & & &  & & & & \\ \hline
test-id5  & & & & & & & & & & & & & & &  & & & &X \\ \hline
test-id6  & & & & & & & & & &X & & & & &  & & & & \\ \hline
test-id7  & & & & & & & & & & & & & & &X  & & & & \\ \hline
test-id8  & & & & & & & & & & & & & &X &X  &X & & & \\ \hline

\hline %
\end{tabular}
}
\caption{Traceability Matrix Showing the Connections Between Non Functional Requirements and their test.}
\label{Table:trace}
\end{table}


\section{Unit Test Description}

The pytest library will be used to complete unit testing for this toolbox. To develop unit tests for the internal functions of the program, we will be creating a corresponding test file for each module. Each test file will contain unit tests for each function within the module. These tests contain a variety of inputs, including those which output the correct transformation as well as inputs that generate errors and exceptions. \\

\noindent We will be using coverage metrics to determine how well-tested our code is. This will be determined through the use of coverage.py, a python library that quickly analyzes code coverage of all modules within a project. We will be aiming for 90\% code coverage per module, ensuring that we adequately test all functions. 
%\wss{Reference your MIS (detailed design document) and explain your overall
 % philosophy for test case selection.}  
%\wss{This section should not be filled in until after the MIS (detailed design
  %document) has been completed.}

\subsection{Unit Testing Scope}

Route choice analysis variable modules will be verified for correct functionality (correct sample inputs output correct sample outputs), but logic of previously existing modules will be assessed for correctness by our supervisor, Dr. Paez.
%\wss{What modules are outside of the scope.  If there are modules that are
%  developed by someone else, then you would say here if you aren't planning on
%  verifying them.  There may also be modules that are part of your software, but
%  have a lower priority for verification than others.  If this is the case,
% 5 explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

This section will be completed once the MIS has been updated and there is greater clarity on specific modules. 

\subsection{Traceability Between Test Cases and Modules}
This section will be completed once the MIS has been updated and there is greater clarity on specific modules. 
%\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section*{Appendix --- Reflection}
General Team:\\

\noindent To implement the verification and validation plan within our project our team will have to learn a few new skills. The team will have to create a standard testing suite and develop a standard testing method that each member of the team will follow. This will be to ensure that all tests are understandable and readable by all members of the group. \\

\noindent The team will also familiarize themselves with the pytest framework which will allow us to create consistent, efficient tests that will test each function in our program. As well as learn system testing techniques such as ---.  The team will also need to create a testing strategy that is appropriate and feasible for the project.\\

\noindent Smita Singh:\\ Smita will be responsible for creating unit test for Route Choice Analysis. She will need understand the inputs and outputs of each of the methods that will be required to perform that specific module. Smita will also be leading the creation of a test strategy. \\

\noindent Moksha Srinivasan:\\ Similar to Smita, Moksha will also be responsible for creating tests for Route Choice Analysis. Moksha will also be responsible for helping set up the standard test suite and implementing CI/CD into our git repository by following the tutorial given by Chris Shankula. \\

\noindent Longwei Ye:\\
Longwei will be responsible for creating unit testing for trip trajectory. Longwei will be responsible for learning about best testing practises through research and will ensure that the team sticks to those practises.\\

\noindent Niyatha Rangarajan:\\
Niyatha will be creating unit testing module for travel episode verification and categorization. Niyatha has been passionate about end to end system testing. She will be researching how to perform relevant system testing by researching industry standards and then be responsible for informing the rest of the team.\\

\noindent Abeer Alyasiri:\\ Abeer will be responsible for learning about different file formats (CSV, XML, JSON, SHP) and the most efficient ways of parsing through and transforming data. This will ensure that modules are well designed and time efficient. She will ensure test cases include all relevant input file formats and malformed data inputs as well. She will learn about these best practices through the use of data parsing python tutorials online as well as researching libraries/prior implementations of open source GIS analysis tools. Through her co-op position, Abeer has significant experience working with various types of data and hence is the most suited member of our team for this task.   \\

\noindent Nicholas Lobo: \\
Nicholas has always been interested in learning about data analysis and normalization. Within the scope of this project, he has taken on the responsibility of learning about GPS data standards to help provide a wide range of inputs for all tests. He will ensure that the preprocessing unit can handle various types of GPS data as well as inform decisions about edge cases related to incorrect data. He can complete this task by consulting academic papers that detail the characteristics of and how to parse GPS data.\\


\end{document}
